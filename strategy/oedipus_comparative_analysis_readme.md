# Oedipus Comparative Analysis Feature
*Design & Engineering Specification*

## Overview

The Comparative Analysis feature transforms Oedipus from an analytics tool into an essential workflow component for AI model evaluation. Instead of analyzing datasets in isolation, this feature enables developers to directly compare model completions, detect performance regressions, and make data-driven decisions about model selection and prompt engineering.

This standalone React application addresses the most critical gap in the current MVP: the lack of actionable comparative insights that developers need to make deployment decisions.

## Problem Statement

**Current State**: Developers manually compare model completions by writing custom scripts, eyeballing CSV files, or building one-off analysis notebooks. This process is time-consuming, error-prone, and doesn't provide statistical rigor.

**Target State**: Developers upload multiple datasets, receive automated statistical comparisons with confidence intervals, and get actionable insights about which models perform better and why.

**Success Criteria**: 
- Time from upload to actionable insight < 2 minutes
- Users compare 3+ datasets in single sessions
- Export/share rate > 50% of completed analyses
- Return usage within 7 days > 60%

## Core Value Proposition

**"Model Performance Detective"** - Answer specific questions developers ask daily:
- "Which model should I deploy?"
- "Did my latest prompt engineering actually improve results?"
- "What prompts consistently cause problems across models?"
- "Is the performance difference statistically significant?"

## Technical Architecture

### Frontend: React Application

**Technology Stack**:
- React 18 with TypeScript
- React Query for server state management
- Zustand for client state management
- Recharts for visualizations (React-native integration)
- Tailwind CSS for styling
- Vite for build tooling

**Component Architecture**:
```
src/
├── components/
│   ├── DatasetUpload/
│   │   ├── DragDropUploader.tsx
│   │   ├── DatasetPreview.tsx
│   │   └── ValidationMessages.tsx
│   ├── ComparisonTable/
│   │   ├── AlignedDataTable.tsx
│   │   ├── FilterControls.tsx
│   │   └── HighlightingEngine.tsx
│   ├── MetricsComparison/
│   │   ├── StatisticalSummary.tsx
│   │   ├── ComparisonCharts.tsx
│   │   └── SignificanceIndicators.tsx
│   ├── StatisticalTests/
│   │   ├── TestResults.tsx
│   │   ├── ConfidenceIntervals.tsx
│   │   └── EffectSizes.tsx
│   ├── InsightsPanel/
│   │   ├── AutoGeneratedInsights.tsx
│   │   ├── RecommendationEngine.tsx
│   │   └── ActionableFindings.tsx
│   └── Export/
│       ├── ReportGenerator.tsx
│       ├── ShareableLinks.tsx
│       └── APIIntegration.tsx
├── hooks/
│   ├── useComparison.ts
│   ├── useAnalysis.ts
│   ├── useStatisticalTests.ts
│   └── useInsightGeneration.ts
├── utils/
│   ├── statisticalTests.ts
│   ├── dataAlignment.ts
│   ├── insightGeneration.ts
│   └── exportFormats.ts
└── types/
    ├── comparison.ts
    ├── analysis.ts
    └── insights.ts
```

### Backend Extensions

**New Database Models**:
```python
class Comparison(Base):
    __tablename__ = "comparisons"
    
    id: UUID = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name: str = Column(String, nullable=False)
    created_at: datetime = Column(DateTime, default=datetime.utcnow)
    
    # Configuration
    datasets: List[UUID] = Column(JSON)  # References to existing datasets
    alignment_key: str = Column(String, default="input_id")
    comparison_config: Dict = Column(JSON)  # User preferences, thresholds
    
    # Results
    statistical_results: Dict = Column(JSON)  # Test outcomes, p-values, effect sizes
    automated_insights: List[str] = Column(JSON)  # Generated findings
    status: str = Column(String, default="pending")  # pending/running/completed/failed

class ComparisonMetric(Base):
    __tablename__ = "comparison_metrics"
    
    id: UUID = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    comparison_id: UUID = Column(UUID(as_uuid=True), ForeignKey("comparisons.id"))
    metric_name: str = Column(String)  # entropy, token_count, custom_score
    dataset_a_value: float = Column(Float)
    dataset_b_value: float = Column(Float)
    statistical_significance: float = Column(Float)  # p-value
    effect_size: float = Column(Float)
    confidence_interval_lower: float = Column(Float)
    confidence_interval_upper: float = Column(Float)
```

**New API Endpoints**:
```python
# Comparison Management
POST   /api/v1/comparisons/create
GET    /api/v1/comparisons/
GET    /api/v1/comparisons/{comparison_id}
DELETE /api/v1/comparisons/{comparison_id}

# Analysis Operations
POST   /api/v1/comparisons/{comparison_id}/analyze
GET    /api/v1/comparisons/{comparison_id}/status
GET    /api/v1/comparisons/{comparison_id}/results

# Export & Sharing
GET    /api/v1/comparisons/{comparison_id}/export/{format}  # json/csv/pdf
POST   /api/v1/comparisons/{comparison_id}/share
GET    /api/v1/comparisons/shared/{share_token}
```

## Feature Specifications

### 1. Dataset Upload & Alignment

**Drag-and-Drop Interface**:
- Support multiple file uploads simultaneously
- Real-time validation and preview
- Automatic schema detection and column mapping
- Clear error messages for format issues

**Data Alignment Engine**:
```typescript
interface AlignmentResult {
  alignedRows: AlignedRow[];
  unmatchedInputs: string[];
  coverageStats: {
    totalInputs: number;
    matchedInputs: number;
    coveragePercentage: number;
  };
}

interface AlignedRow {
  inputId: string;
  inputText: string;
  completions: {
    [datasetName: string]: string[] | null;
  };
  metadata: {
    [datasetName: string]: Record<string, any>;
  };
}
```

**Validation Rules**:
- Minimum 10 aligned prompts for statistical significance
- Maximum 50,000 rows per dataset (performance constraint)
- Required fields: prompt identifier and completion content
- Optional fields: model version, timestamp, custom scores

### 2. Side-by-Side Comparison Interface

**Interactive Data Table**:
- Virtualized scrolling for large datasets
- Column-wise sorting and filtering
- Highlighting for significant differences
- Quick navigation to outliers and edge cases

**Comparison Controls**:
- Dataset selection/deselection
- Metric threshold adjustments
- Statistical significance levels (0.05, 0.01, 0.001)
- Export subset selections

### 3. Statistical Analysis Engine

**Core Statistical Tests**:
```typescript
interface StatisticalTest {
  name: string;
  pValue: number;
  effectSize: number;
  confidenceInterval: [number, number];
  interpretation: string;
  recommendation: string;
}

// Implemented tests:
- MannWhitneyU: "Is the median significantly different?"
- TTest: "Are the means significantly different?"
- KolmogorovSmirnov: "Are the distributions different?"
- ChiSquare: "Are categorical outcomes different?"
- EffectSizeCalculation: "How large is the practical difference?"
```

**Automated Significance Detection**:
- Highlight metrics with p < 0.05
- Calculate and display effect sizes (Cohen's d)
- Generate confidence intervals for all comparisons
- Flag practical significance vs statistical significance

### 4. Insight Generation System

**Automated Insights**:
```typescript
interface GeneratedInsight {
  type: 'performance' | 'quality' | 'consistency' | 'regression';
  severity: 'high' | 'medium' | 'low';
  title: string;
  description: string;
  evidence: {
    metric: string;
    value: number;
    comparison: string;
    confidence: number;
  };
  recommendation: string;
  affectedInputs: string[];
}
```

**Insight Categories**:
- **Performance Differences**: "Model B generates 34% more diverse responses (p<0.001)"
- **Quality Regressions**: "3 prompts consistently produce lower quality completions in Model A"
- **Consistency Issues**: "Model C shows 2x higher variance in response length"
- **Edge Case Detection**: "Inputs >200 tokens show degraded performance across all models"

### 5. Visualization Dashboard

**Comparison Charts**:
- Box plots for metric distributions
- Scatter plots for correlation analysis
- Bar charts for categorical comparisons
- Time series for version-over-time analysis

**Interactive Features**:
- Click-through from chart to specific data points
- Dynamic filtering based on chart selections
- Real-time updates during analysis
- Customizable chart configurations

### 6. Export & Collaboration

**Report Generation**:
```typescript
interface ComparisonReport {
  metadata: {
    comparisonName: string;
    datasets: string[];
    generatedAt: Date;
    analysisConfig: any;
  };
  executiveSummary: {
    keyFindings: string[];
    recommendations: string[];
    statisticalSummary: any;
  };
  detailedResults: {
    metrics: StatisticalTest[];
    insights: GeneratedInsight[];
    rawData: any[];
  };
  visualizations: {
    charts: ChartConfig[];
    exportUrls: string[];
  };
}
```

**Export Formats**:
- **PDF Report**: Executive summary with key charts
- **CSV Data**: Raw comparison results for further analysis
- **JSON API**: Programmatic access for integrations
- **Shareable Links**: Public URLs for collaboration

## Development Roadmap

### Phase 1: Core Infrastructure (Week 1-2)
**Backend Development**:
- Extend existing FastAPI with comparison endpoints
- Implement Comparison and ComparisonMetric models
- Build data alignment algorithms
- Create basic statistical test implementations

**Frontend Bootstrap**:
- React app initialization with TypeScript
- Basic routing and layout structure
- Dataset upload component
- API integration layer

**Deliverables**:
- Functional upload interface
- Backend API for comparison creation
- Basic data alignment and validation

### Phase 2: Statistical Analysis (Week 2-3)
**Analysis Engine**:
- Implement core statistical tests
- Build automated insight generation
- Create confidence interval calculations
- Add significance detection algorithms

**UI Components**:
- Comparison table with highlighting
- Statistical results display
- Basic chart implementations
- Loading states and error handling

**Deliverables**:
- Working statistical comparison pipeline
- Interactive comparison interface
- Automated insight generation

### Phase 3: Polish & Integration (Week 3-4)
**User Experience**:
- Responsive design implementation
- Advanced filtering and search
- Export functionality
- Share/collaboration features

**Performance & Reliability**:
- Optimization for large datasets
- Error boundary implementation
- Comprehensive testing suite
- Documentation and examples

**Deliverables**:
- Production-ready application
- Full export/share functionality
- Performance benchmarks met

## Technical Considerations

### Performance Constraints
- **Frontend**: Virtualized tables for >1000 rows
- **Backend**: Async processing for statistical calculations
- **Database**: Indexed queries on comparison lookups
- **Memory**: Streaming processing for large datasets

### Error Handling Strategy
```typescript
// Graceful degradation patterns
const AnalysisResult = {
  // Always provide partial results
  partialMetrics: Metric[];
  failedCalculations: string[];
  warnings: string[];
  // Clear error messaging
  errorContext: {
    userFacingMessage: string;
    technicalDetails: string;
    suggestedActions: string[];
  };
}
```

### Security & Privacy
- **Data Isolation**: Comparisons scoped to user/organization
- **Temporary Storage**: Option to delete datasets after analysis
- **Secure Sharing**: Time-limited, token-based public links
- **Input Validation**: Sanitization of all user uploads

### Integration Points
- **CI/CD Hooks**: Webhook endpoints for automated comparisons
- **API Access**: RESTful endpoints for programmatic usage
- **Export Integration**: Direct connections to MLflow, W&B
- **Notification Systems**: Slack/email alerts for significant changes

## Success Metrics

### User Engagement
- **Time to Value**: Upload to insight < 2 minutes
- **Session Depth**: 3+ datasets compared per session
- **Feature Adoption**: Export usage rate > 50%
- **Retention**: 7-day return rate > 60%

### Technical Performance
- **Processing Speed**: 10K rows analyzed in < 30 seconds
- **UI Responsiveness**: Table interactions < 100ms
- **Error Rate**: < 1% failed analyses
- **Uptime**: 99.5% availability

### Business Impact
- **User Feedback**: NPS > 50 from beta users
- **Workflow Integration**: 30%+ users integrate into CI/CD
- **Feature Requests**: Comparative analysis cited as primary value
- **Conversion**: 20%+ of trial users upgrade after using comparison

## Risk Mitigation

### Technical Risks
- **Statistical Complexity**: Partner with statistics experts for validation
- **Performance Scaling**: Implement progressive loading and caching
- **Data Quality**: Robust validation with clear user feedback

### Product Risks
- **Feature Complexity**: Start with simple comparisons, add sophistication
- **User Adoption**: Focus on solving specific, painful use cases first
- **Competitive Response**: Build network effects through collaboration features

### Business Risks
- **Market Timing**: Validate demand through beta user feedback
- **Resource Allocation**: Maintain lean MVP approach with clear success gates
- **Technical Debt**: Plan React migration path from current Streamlit codebase

This specification provides the foundation for building a comparative analysis feature that addresses the core gap in the current Oedipus MVP while positioning the product for future growth and developer adoption.